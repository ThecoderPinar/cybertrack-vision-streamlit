import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from sklearn.ensemble import RandomForestClassifier, IsolationForest
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

st.set_page_config(
    page_title="ğŸ¤– Makine Ã–ÄŸrenmesi",
    page_icon="ğŸ¤–",
    layout="wide"
)

st.markdown("# ğŸ¤– Makine Ã–ÄŸrenmesi Modelleri")

@st.cache_data
def load_data():
    df = pd.read_csv('data/cybertrack_mock_dataset.csv')
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    
    # Risk kategorilerini dÃ¼zenle
    risk_mapping = {
        'DÃ¼ÅŸÃ¼k': 'Low',
        'Orta': 'Medium', 
        'YÃ¼ksek': 'High'
    }
    df['risk_category_en'] = df['risk_category'].map(risk_mapping)
    
    # Zaman Ã¶zelliklerini ekle
    df['hour'] = df['timestamp'].dt.hour
    df['day_of_week'] = df['timestamp'].dt.dayofweek
    df['month'] = df['timestamp'].dt.month
    
    return df

def prepare_features(df):
    """Makine Ã¶ÄŸrenmesi iÃ§in Ã¶zellik hazÄ±rlama"""
    # Kategorik deÄŸiÅŸkenleri encode et
    le_country = LabelEncoder()
    le_isp = LabelEncoder()
    
    df_encoded = df.copy()
    df_encoded['country_encoded'] = le_country.fit_transform(df['country'])
    df_encoded['isp_encoded'] = le_isp.fit_transform(df['isp'])
    
    # Ã–zellik matrisi
    features = ['latitude', 'longitude', 'hour', 'day_of_week', 'month', 
                'country_encoded', 'isp_encoded']
    
    X = df_encoded[features]
    y_regression = df_encoded['risk']
    y_classification = df_encoded['risk_category_en']
    
    return X, y_regression, y_classification, le_country, le_isp

def risk_prediction_model(df):
    """Risk tahmin modeli"""
    st.markdown("## ğŸ¯ Risk Skoru Tahmin Modeli")
    
    X, y_regression, y_classification, le_country, le_isp = prepare_features(df)
    
    tab1, tab2 = st.tabs(["ğŸ“Š Regresyon Modeli", "ğŸ·ï¸ SÄ±nÄ±flandÄ±rma Modeli"])
    
    with tab1:
        st.markdown("### Regresyon - Risk Skoru Tahmini")
        
        # Model eÄŸitimi
        X_train, X_test, y_train, y_test = train_test_split(X, y_regression, test_size=0.2, random_state=42)
        
        from sklearn.ensemble import RandomForestRegressor
        rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)
        rf_regressor.fit(X_train, y_train)
        
        # Tahminler
        y_pred = rf_regressor.predict(X_test)
        
        # Model performansÄ±
        from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
        
        mae = mean_absolute_error(y_test, y_pred)
        mse = mean_squared_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Ortalama Mutlak Hata (MAE)", f"{mae:.2f}")
        with col2:
            st.metric("RÂ² Skoru", f"{r2:.3f}")
        with col3:
            st.metric("RMSE", f"{np.sqrt(mse):.2f}")
        
        # GerÃ§ek vs Tahmin grafiÄŸi
        col1, col2 = st.columns(2)
        
        with col1:
            fig = px.scatter(
                x=y_test, 
                y=y_pred,
                title="GerÃ§ek vs Tahmin Edilen Risk SkorlarÄ±",
                labels={'x': 'GerÃ§ek Risk', 'y': 'Tahmin Edilen Risk'},
                opacity=0.6
            )
            
            # Perfect prediction line
            min_val = min(y_test.min(), y_pred.min())
            max_val = max(y_test.max(), y_pred.max())
            fig.add_trace(go.Scatter(
                x=[min_val, max_val],
                y=[min_val, max_val],
                mode='lines',
                name='MÃ¼kemmel Tahmin',
                line=dict(dash='dash', color='red')
            ))
            
            fig.update_layout(height=400)
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # Feature importance
            feature_importance = pd.DataFrame({
                'feature': X.columns,
                'importance': rf_regressor.feature_importances_
            }).sort_values('importance', ascending=False)
            
            fig = px.bar(
                feature_importance,
                x='importance',
                y='feature',
                orientation='h',
                title="Ã–zellik Ã–nem SÄ±ralamasÄ±",
                labels={'importance': 'Ã–nem Skoru', 'feature': 'Ã–zellik'}
            )
            fig.update_layout(height=400)
            st.plotly_chart(fig, use_container_width=True)
    
    with tab2:
        st.markdown("### SÄ±nÄ±flandÄ±rma - Risk Kategorisi Tahmini")
        
        # Model eÄŸitimi
        X_train, X_test, y_train_class, y_test_class = train_test_split(X, y_classification, test_size=0.2, random_state=42)
        
        rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
        rf_classifier.fit(X_train, y_train_class)
        
        # Tahminler
        y_pred_class = rf_classifier.predict(X_test)
        
        # Model performansÄ±
        from sklearn.metrics import accuracy_score, precision_recall_fscore_support
        
        accuracy = accuracy_score(y_test_class, y_pred_class)
        precision, recall, f1, _ = precision_recall_fscore_support(y_test_class, y_pred_class, average='weighted')
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("DoÄŸruluk (Accuracy)", f"{accuracy:.3f}")
        with col2:
            st.metric("Kesinlik (Precision)", f"{precision:.3f}")
        with col3:
            st.metric("Geri Ã‡aÄŸÄ±rma (Recall)", f"{recall:.3f}")
        
        # Confusion Matrix
        col1, col2 = st.columns(2)
        
        with col1:
            cm = confusion_matrix(y_test_class, y_pred_class)
            
            fig = px.imshow(
                cm,
                text_auto=True,
                aspect="auto",
                title="KarÄ±ÅŸÄ±klÄ±k Matrisi",
                labels={'x': 'Tahmin Edilen', 'y': 'GerÃ§ek'},
                x=['High', 'Low', 'Medium'],
                y=['High', 'Low', 'Medium']
            )
            fig.update_layout(height=400)
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # SÄ±nÄ±f bazlÄ± metrikler
            report = classification_report(y_test_class, y_pred_class, output_dict=True)
            
            classes = ['High', 'Low', 'Medium']
            metrics_df = pd.DataFrame({
                'SÄ±nÄ±f': classes,
                'Precision': [report[cls]['precision'] for cls in classes],
                'Recall': [report[cls]['recall'] for cls in classes],
                'F1-Score': [report[cls]['f1-score'] for cls in classes]
            })
            
            fig = px.bar(
                metrics_df.melt(id_vars='SÄ±nÄ±f', var_name='Metrik', value_name='Skor'),
                x='SÄ±nÄ±f',
                y='Skor',
                color='Metrik',
                title="SÄ±nÄ±f BazlÄ± Model PerformansÄ±",
                barmode='group'
            )
            fig.update_layout(height=400)
            st.plotly_chart(fig, use_container_width=True)

def anomaly_detection(df):
    """Anomali tespiti"""
    st.markdown("## ğŸš¨ Anomali Tespit Modeli")
    
    # Ã–zellik hazÄ±rlama
    X, _, _, _, _ = prepare_features(df)
    
    # Isolation Forest modeli
    isolation_forest = IsolationForest(contamination=0.1, random_state=42)
    anomaly_labels = isolation_forest.fit_predict(X)
    
    # Anomali skorlarÄ±
    anomaly_scores = isolation_forest.decision_function(X)
    
    # SonuÃ§larÄ± DataFrame'e ekle
    df_anomaly = df.copy()
    df_anomaly['anomaly'] = anomaly_labels
    df_anomaly['anomaly_score'] = anomaly_scores
    df_anomaly['is_anomaly'] = df_anomaly['anomaly'] == -1
    
    # Anomali istatistikleri
    total_anomalies = (anomaly_labels == -1).sum()
    anomaly_percentage = (total_anomalies / len(df)) * 100
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Toplam Anomali", total_anomalies)
    with col2:
        st.metric("Anomali OranÄ±", f"{anomaly_percentage:.1f}%")
    with col3:
        avg_risk_anomaly = df_anomaly[df_anomaly['is_anomaly']]['risk'].mean()
        st.metric("Anomali Ortalama Risk", f"{avg_risk_anomaly:.1f}")
    
    tab1, tab2, tab3 = st.tabs(["ğŸ“Š Anomali DaÄŸÄ±lÄ±mÄ±", "ğŸ—ºï¸ CoÄŸrafi GÃ¶rÃ¼nÃ¼m", "ğŸ“‹ Anomali DetaylarÄ±"])
    
    with tab1:
        col1, col2 = st.columns(2)
        
        with col1:
            # Anomali vs Normal daÄŸÄ±lÄ±mÄ±
            fig = px.histogram(
                df_anomaly,
                x='risk',
                color='is_anomaly',
                nbins=30,
                title="Risk DaÄŸÄ±lÄ±mÄ±: Normal vs Anomali",
                labels={'is_anomaly': 'Anomali', 'risk': 'Risk Skoru'},
                color_discrete_map={True: 'red', False: 'blue'}
            )
            fig.update_layout(height=400)
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # Anomali skorlarÄ± daÄŸÄ±lÄ±mÄ±
            fig = px.histogram(
                df_anomaly,
                x='anomaly_score',
                nbins=30,
                title="Anomali SkorlarÄ± DaÄŸÄ±lÄ±mÄ±",
                labels={'anomaly_score': 'Anomali Skoru'}
            )
            fig.add_vline(
                x=df_anomaly[df_anomaly['is_anomaly']]['anomaly_score'].max(),
                line_dash="dash",
                line_color="red",
                annotation_text="Anomali EÅŸiÄŸi"
            )
            fig.update_layout(height=400)
            st.plotly_chart(fig, use_container_width=True)
    
    with tab2:
        # CoÄŸrafi anomali haritasÄ±
        anomalies_geo = df_anomaly[df_anomaly['is_anomaly']]
        
        if not anomalies_geo.empty:
            fig = px.scatter_mapbox(
                anomalies_geo,
                lat='latitude',
                lon='longitude',
                size='risk',
                color='anomaly_score',
                hover_data=['country', 'risk', 'isp'],
                title="CoÄŸrafi Anomali DaÄŸÄ±lÄ±mÄ±",
                mapbox_style='open-street-map',
                zoom=1,
                height=600
            )
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("CoÄŸrafi anomali bulunamadÄ±.")
    
    with tab3:
        # En yÃ¼ksek anomali skorlarÄ±na sahip kayÄ±tlar
        st.markdown("### ğŸ” En YÃ¼ksek Anomali SkorlarÄ±")
        
        top_anomalies = df_anomaly[df_anomaly['is_anomaly']].nsmallest(10, 'anomaly_score')[
            ['timestamp', 'ip', 'country', 'risk', 'isp', 'anomaly_score']
        ]
        
        if not top_anomalies.empty:
            st.dataframe(top_anomalies, use_container_width=True)
        else:
            st.info("Anomali bulunamadÄ±.")

def clustering_analysis(df):
    """KÃ¼meleme analizi"""
    st.markdown("## ğŸ¯ KÃ¼meleme Analizi")
    
    from sklearn.cluster import KMeans
    from sklearn.preprocessing import StandardScaler
    
    # Ã–zellik seÃ§imi ve hazÄ±rlama
    features_for_clustering = ['risk', 'latitude', 'longitude']
    X_cluster = df[features_for_clustering].dropna()
    
    # Veriyi standartlaÅŸtÄ±r
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_cluster)
    
    # Optimal kÃ¼me sayÄ±sÄ±nÄ± bul
    col1, col2 = st.columns(2)
    
    with col1:
        # Elbow method
        inertias = []
        k_range = range(2, 11)
        
        for k in k_range:
            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
            kmeans.fit(X_scaled)
            inertias.append(kmeans.inertia_)
        
        fig = px.line(
            x=list(k_range),
            y=inertias,
            title="Optimal KÃ¼me SayÄ±sÄ± (Elbow Method)",
            labels={'x': 'KÃ¼me SayÄ±sÄ±', 'y': 'Inertia'},
            markers=True
        )
        fig.update_layout(height=400)
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        # KÃ¼me sayÄ±sÄ± seÃ§imi
        n_clusters = st.selectbox("KÃ¼me SayÄ±sÄ±nÄ± SeÃ§in", options=list(k_range), index=2)
        
        # KMeans uygula
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        cluster_labels = kmeans.fit_predict(X_scaled)
        
        # KÃ¼me daÄŸÄ±lÄ±mÄ±
        cluster_counts = pd.Series(cluster_labels).value_counts().sort_index()
        
        fig = px.bar(
            x=[f"KÃ¼me {i}" for i in cluster_counts.index],
            y=cluster_counts.values,
            title="KÃ¼me DaÄŸÄ±lÄ±mÄ±",
            labels={'x': 'KÃ¼me', 'y': 'KayÄ±t SayÄ±sÄ±'}
        )
        fig.update_layout(height=300)
        st.plotly_chart(fig, use_container_width=True)
    
    # KÃ¼meleme sonuÃ§larÄ±nÄ± gÃ¶rselleÅŸtir
    X_cluster_with_labels = X_cluster.copy()
    X_cluster_with_labels['Cluster'] = cluster_labels
    
    tab1, tab2 = st.tabs(["ğŸ¨ 3D GÃ¶rÃ¼nÃ¼m", "ğŸ“Š KÃ¼me Analizi"])
    
    with tab1:
        # 3D scatter plot
        fig = px.scatter_3d(
            X_cluster_with_labels,
            x='latitude',
            y='longitude',
            z='risk',
            color='Cluster',
            title="3D KÃ¼meleme SonuÃ§larÄ±",
            labels={'latitude': 'Enlem', 'longitude': 'Boylam', 'risk': 'Risk Skoru'}
        )
        fig.update_layout(height=600)
        st.plotly_chart(fig, use_container_width=True)
    
    with tab2:
        # KÃ¼me Ã¶zellikleri
        cluster_summary = X_cluster_with_labels.groupby('Cluster').agg({
            'risk': ['mean', 'std', 'count'],
            'latitude': 'mean',
            'longitude': 'mean'
        }).round(2)
        
        cluster_summary.columns = ['Ortalama Risk', 'Risk Std', 'KayÄ±t SayÄ±sÄ±', 'Ortalama Enlem', 'Ortalama Boylam']
        st.dataframe(cluster_summary, use_container_width=True)
        
        # Her kÃ¼menin karakteristikleri
        st.markdown("### ğŸ” KÃ¼me Karakteristikleri")
        
        for i in range(n_clusters):
            cluster_data = X_cluster_with_labels[X_cluster_with_labels['Cluster'] == i]
            avg_risk = cluster_data['risk'].mean()
            count = len(cluster_data)
            
            risk_level = "ğŸ”´ YÃ¼ksek" if avg_risk > 60 else "ğŸŸ¡ Orta" if avg_risk > 30 else "ğŸŸ¢ DÃ¼ÅŸÃ¼k"
            
            st.write(f"**KÃ¼me {i}**: {count} kayÄ±t, Ortalama Risk: {avg_risk:.1f} {risk_level}")

def predictive_modeling(df):
    """Tahminsel modelleme"""
    st.markdown("## ğŸ”® Tahminsel Modelleme")
    
    # Gelecek saldÄ±rÄ± tahmini iÃ§in zaman serisi analizi
    daily_attacks = df.groupby(df['timestamp'].dt.date).size().reset_index(name='attacks')
    daily_attacks['date'] = pd.to_datetime(daily_attacks['timestamp'])
    daily_attacks['days_from_start'] = (daily_attacks['date'] - daily_attacks['date'].min()).dt.days
    
    # Basit linear regression
    from sklearn.linear_model import LinearRegression
    
    X_time = daily_attacks[['days_from_start']]
    y_time = daily_attacks['attacks']
    
    lr_model = LinearRegression()
    lr_model.fit(X_time, y_time)
    
    # Gelecek 7 gÃ¼nlÃ¼k tahmin
    future_days = np.arange(daily_attacks['days_from_start'].max() + 1, 
                           daily_attacks['days_from_start'].max() + 8).reshape(-1, 1)
    future_predictions = lr_model.predict(future_days)
    
    col1, col2 = st.columns(2)
    
    with col1:
        # Trend ve tahmin grafiÄŸi
        fig = go.Figure()
        
        # GerÃ§ek veriler
        fig.add_trace(go.Scatter(
            x=daily_attacks['date'],
            y=daily_attacks['attacks'],
            mode='lines+markers',
            name='GerÃ§ek Veriler',
            line=dict(color='blue')
        ))
        
        # Tahminler
        future_dates = [daily_attacks['date'].max() + pd.Timedelta(days=i) for i in range(1, 8)]
        fig.add_trace(go.Scatter(
            x=future_dates,
            y=future_predictions,
            mode='lines+markers',
            name='7 GÃ¼nlÃ¼k Tahmin',
            line=dict(color='red', dash='dash'),
            marker=dict(symbol='diamond', size=8)
        ))
        
        fig.update_layout(
            title="SaldÄ±rÄ± Trendi ve Gelecek Tahmini",
            xaxis_title="Tarih",
            yaxis_title="GÃ¼nlÃ¼k SaldÄ±rÄ± SayÄ±sÄ±",
            height=400
        )
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        # Model performansÄ± ve metrikler
        train_predictions = lr_model.predict(X_time)
        r2 = r2_score(y_time, train_predictions)
        
        st.markdown("### ğŸ“Š Model PerformansÄ±")
        st.metric("RÂ² Skoru", f"{r2:.3f}")
        st.metric("Trend KatsayÄ±sÄ±", f"{lr_model.coef_[0]:.3f}")
        
        # Gelecek tahminleri tablosu
        st.markdown("### ğŸ“… 7 GÃ¼nlÃ¼k Tahmin")
        future_df = pd.DataFrame({
            'Tarih': [d.strftime('%Y-%m-%d') for d in future_dates],
            'Tahmini SaldÄ±rÄ±': [f"{p:.0f}" for p in future_predictions]
        })
        st.dataframe(future_df, use_container_width=True)

def model_insights(df):
    """Model iÃ§gÃ¶rÃ¼leri"""
    st.markdown("## ğŸ’¡ Model Ä°Ã§gÃ¶rÃ¼leri ve Ã–neriler")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("### ğŸ¯ Risk FaktÃ¶rleri")
        
        # En yÃ¼ksek risk faktÃ¶rlerini belirle
        high_risk_countries = df.groupby('country')['risk'].mean().sort_values(ascending=False).head(5)
        high_risk_isps = df.groupby('isp')['risk'].mean().sort_values(ascending=False).head(5)
        
        st.markdown("**En Riskli Ãœlkeler:**")
        for country, risk in high_risk_countries.items():
            st.write(f"ğŸš© {country}: {risk:.1f}")
        
        st.markdown("**En Riskli ISP'ler:**")
        for isp, risk in high_risk_isps.items():
            st.write(f"ğŸŒ {isp}: {risk:.1f}")
    
    with col2:
        st.markdown("### ğŸ›¡ï¸ GÃ¼venlik Ã–nerileri")
        
        # Zaman bazlÄ± Ã¶neriler
        risky_hours = df.groupby('hour')['risk'].mean().sort_values(ascending=False).head(3)
        
        st.markdown("**Zaman BazlÄ± Ã–neriler:**")
        for hour, risk in risky_hours.items():
            st.write(f"â° Saat {hour}:00 - YÃ¼ksek risk ({risk:.1f})")
        
        st.markdown("**Genel Ã–neriler:**")
        st.write("ğŸ” Anomali tespit sistemini aktif tutun")
        st.write("ğŸ“Š Risk skorlarÄ± 70'in Ã¼zerindeki IP'leri izleyin")
        st.write("ğŸŒ CoÄŸrafi filtreleme uygulayÄ±n")
        st.write("ğŸ• YoÄŸun saatlerde ekstra gÃ¼venlik Ã¶nlemleri alÄ±n")

# Ana fonksiyon
def main():
    df = load_data()
    
    # Model seÃ§imi
    st.sidebar.markdown("## ğŸ¤– Model SeÃ§imi")
    model_type = st.sidebar.selectbox(
        "Analiz tÃ¼rÃ¼nÃ¼ seÃ§in:",
        ["Risk Tahmin Modeli", "Anomali Tespiti", "KÃ¼meleme Analizi", "Tahminsel Modelleme", "Model Ä°Ã§gÃ¶rÃ¼leri"]
    )
    
    if model_type == "Risk Tahmin Modeli":
        risk_prediction_model(df)
    elif model_type == "Anomali Tespiti":
        anomaly_detection(df)
    elif model_type == "KÃ¼meleme Analizi":
        clustering_analysis(df)
    elif model_type == "Tahminsel Modelleme":
        predictive_modeling(df)
    else:
        model_insights(df)

if __name__ == "__main__":
    main()
